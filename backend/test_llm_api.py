#!/usr/bin/env python3
"""
æµ‹è¯•LLM APIè¿æ¥çš„è„šæœ¬
"""
import os
import requests
import json
from services.llm_client import LLMClient

def test_direct_api():
    """ç›´æ¥æµ‹è¯•APIè°ƒç”¨"""
    print("ğŸ” ç›´æ¥æµ‹è¯•LLM APIè°ƒç”¨...")
    
    api_base = os.getenv('LLM_API_BASE', "http://v2.open.venus.oa.com/llmproxy")
    api_key = os.getenv('LLM_API_KEY', "xxBZykeTGIVeqyGNaxNoMDro@2468")
    
    print(f"ğŸ“ API Base: {api_base}")
    print(f"ğŸ”‘ API Key: {api_key[:10]}...")
    
    url = f"{api_base}/chat/completions"
    print(f"ğŸŒ å®Œæ•´URL: {url}")
    
    data = {
        "model": "deepseek-v3-local-II",
        "messages": [{"role": "user", "content": "Hello, æµ‹è¯•ä¸€ä¸‹APIè¿æ¥"}],
        "max_tokens": 100,
        "temperature": 0.1
    }
    
    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json'
    }
    
    try:
        print("ğŸ“¤ å‘é€APIè¯·æ±‚...")
        response = requests.post(url, json=data, headers=headers, timeout=30)
        
        print(f"ğŸ“Š çŠ¶æ€ç : {response.status_code}")
        print(f"ğŸ“‹ å“åº”å¤´: {dict(response.headers)}")
        print(f"ğŸ“„ å“åº”å†…å®¹: {response.text[:500]}")
        
        if response.status_code == 200:
            result = response.json()
            if 'choices' in result:
                print(f"âœ… APIè°ƒç”¨æˆåŠŸï¼å›ç­”: {result['choices'][0]['message']['content']}")
            else:
                print(f"âŒ å“åº”æ ¼å¼é”™è¯¯: {result}")
        else:
            print(f"âŒ APIè°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            
    except Exception as e:
        print(f"ğŸ’¥ è¯·æ±‚å¼‚å¸¸: {str(e)}")
        import traceback
        traceback.print_exc()

def test_llm_client():
    """æµ‹è¯•LLMå®¢æˆ·ç«¯"""
    print("\nğŸ” æµ‹è¯•LLMå®¢æˆ·ç«¯...")
    
    try:
        client = LLMClient()
        print(f"âœ… LLMå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
        print(f"ğŸ“ API Base: {client.api_base}")
        print(f"ğŸ”‘ API Key: {client.api_key[:10]}...")
        
        response = client.dialog("ä½ å¥½ï¼Œè¯·å›ç­”æµ‹è¯•é—®é¢˜", task_type='evaluation')
        print(f"âœ… LLMå®¢æˆ·ç«¯è°ƒç”¨æˆåŠŸï¼å›ç­”: {response[:100]}...")
        
    except Exception as e:
        print(f"ğŸ’¥ LLMå®¢æˆ·ç«¯è°ƒç”¨å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹LLM APIè¿æ¥æµ‹è¯•...\n")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    print("ğŸ“‹ ç¯å¢ƒå˜é‡æ£€æŸ¥:")
    for key in ['LLM_API_BASE', 'LLM_API_KEY', 'LLM_MODEL']:
        value = os.getenv(key, 'Not Set')
        if key == 'LLM_API_KEY' and value != 'Not Set':
            value = value[:10] + "..."
        print(f"   {key}: {value}")
    print()
    
    # æµ‹è¯•ç›´æ¥APIè°ƒç”¨
    test_direct_api()
    
    # æµ‹è¯•LLMå®¢æˆ·ç«¯
    test_llm_client()
    
    print("\nğŸ¯ æµ‹è¯•å®Œæˆ") 
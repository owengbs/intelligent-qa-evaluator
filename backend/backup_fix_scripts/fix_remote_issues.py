#!/usr/bin/env python3
"""
ä¿®å¤è¿œç«¯ç¯å¢ƒé—®é¢˜çš„è„šæœ¬
ä¸»è¦è§£å†³ï¼š
1. æ•°æ®åº“æ—¥æœŸå­—æ®µæ ¼å¼ä¸ä¸€è‡´é—®é¢˜
2. äººå·¥è¯„ä¼°è®°å½•ä¸å­˜åœ¨é—®é¢˜
3. æ•°æ®åº“è¡¨ç»“æ„ç¼ºå¤±é—®é¢˜
"""

import os
import sys
import sqlite3
from datetime import datetime
import json

# æ·»åŠ çˆ¶ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def check_database_structure():
    """æ£€æŸ¥æ•°æ®åº“è¡¨ç»“æ„"""
    print("ğŸ” æ£€æŸ¥æ•°æ®åº“è¡¨ç»“æ„...")
    
    db_path = os.path.join('data', 'qa_evaluator.db')
    if not os.path.exists(db_path):
        print("âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # æ£€æŸ¥evaluation_historyè¡¨ç»“æ„
        cursor.execute("PRAGMA table_info(evaluation_history)")
        columns = cursor.fetchall()
        
        print(f"âœ… evaluation_historyè¡¨æœ‰ {len(columns)} ä¸ªå­—æ®µ")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰äººå·¥è¯„ä¼°å­—æ®µ
        column_names = [col[1] for col in columns]
        human_eval_fields = [
            'human_total_score',
            'human_dimensions_json', 
            'human_reasoning',
            'human_evaluation_by',
            'human_evaluation_time',
            'is_human_modified'
        ]
        
        missing_fields = []
        for field in human_eval_fields:
            if field not in column_names:
                missing_fields.append(field)
        
        if missing_fields:
            print(f"âŒ ç¼ºå°‘äººå·¥è¯„ä¼°å­—æ®µ: {missing_fields}")
            return False
        else:
            print("âœ… äººå·¥è¯„ä¼°å­—æ®µå®Œæ•´")
        
        conn.close()
        return True
        
    except Exception as e:
        print(f"âŒ æ£€æŸ¥æ•°æ®åº“ç»“æ„å¤±è´¥: {e}")
        return False

def fix_date_format_issues():
    """ä¿®å¤æ—¥æœŸæ ¼å¼é—®é¢˜"""
    print("\nğŸ”§ ä¿®å¤æ—¥æœŸæ ¼å¼é—®é¢˜...")
    
    try:
        from app import app
        from models.classification import db, EvaluationHistory
        
        with app.app_context():
            # è·å–æ‰€æœ‰è¯„ä¼°è®°å½•
            evaluations = EvaluationHistory.query.all()
            
            fixed_count = 0
            for evaluation in evaluations:
                try:
                    # å°è¯•è°ƒç”¨to_dictæ–¹æ³•ï¼Œå¦‚æœå‡ºé”™å°±ä¿®å¤
                    evaluation.to_dict()
                except Exception as e:
                    print(f"ä¿®å¤è®°å½• {evaluation.id} çš„æ—¥æœŸæ ¼å¼é—®é¢˜...")
                    
                    # ä¿®å¤å¯èƒ½çš„æ—¥æœŸå­—æ®µé—®é¢˜
                    if evaluation.question_time and isinstance(evaluation.question_time, str):
                        try:
                            evaluation.question_time = datetime.fromisoformat(evaluation.question_time.replace('Z', '+00:00'))
                        except:
                            evaluation.question_time = None
                    
                    if evaluation.human_evaluation_time and isinstance(evaluation.human_evaluation_time, str):
                        try:
                            evaluation.human_evaluation_time = datetime.fromisoformat(evaluation.human_evaluation_time.replace('Z', '+00:00'))
                        except:
                            evaluation.human_evaluation_time = None
                    
                    if evaluation.created_at and isinstance(evaluation.created_at, str):
                        try:
                            evaluation.created_at = datetime.fromisoformat(evaluation.created_at.replace('Z', '+00:00'))
                        except:
                            evaluation.created_at = datetime.utcnow()
                    
                    if evaluation.updated_at and isinstance(evaluation.updated_at, str):
                        try:
                            evaluation.updated_at = datetime.fromisoformat(evaluation.updated_at.replace('Z', '+00:00'))
                        except:
                            evaluation.updated_at = datetime.utcnow()
                    
                    fixed_count += 1
            
            if fixed_count > 0:
                db.session.commit()
                print(f"âœ… ä¿®å¤äº† {fixed_count} æ¡è®°å½•çš„æ—¥æœŸæ ¼å¼")
            else:
                print("âœ… æ‰€æœ‰è®°å½•çš„æ—¥æœŸæ ¼å¼æ­£å¸¸")
            
            return True
            
    except Exception as e:
        print(f"âŒ ä¿®å¤æ—¥æœŸæ ¼å¼å¤±è´¥: {e}")
        return False

def verify_human_evaluation_api():
    """éªŒè¯äººå·¥è¯„ä¼°APIåŠŸèƒ½"""
    print("\nğŸ§ª éªŒè¯äººå·¥è¯„ä¼°APIåŠŸèƒ½...")
    
    try:
        from app import app
        from models.classification import EvaluationHistory
        
        with app.app_context():
            # æ‰¾ä¸€ä¸ªè¯„ä¼°è®°å½•è¿›è¡Œæµ‹è¯•
            evaluation = EvaluationHistory.query.first()
            
            if not evaluation:
                print("âš ï¸  æ²¡æœ‰è¯„ä¼°è®°å½•å¯ç”¨äºæµ‹è¯•")
                return False
            
            print(f"âœ… æ‰¾åˆ°æµ‹è¯•è®°å½• ID: {evaluation.id}")
            
            # æµ‹è¯•to_dictæ–¹æ³•
            try:
                data = evaluation.to_dict()
                print("âœ… to_dictæ–¹æ³•æ­£å¸¸")
            except Exception as e:
                print(f"âŒ to_dictæ–¹æ³•å¤±è´¥: {e}")
                return False
            
            # æµ‹è¯•äººå·¥è¯„ä¼°æ›´æ–°
            try:
                evaluation.human_total_score = 8.0
                evaluation.human_reasoning = "æµ‹è¯•äººå·¥è¯„ä¼°"
                evaluation.human_evaluation_by = "æµ‹è¯•ç”¨æˆ·"
                evaluation.human_evaluation_time = datetime.utcnow()
                evaluation.is_human_modified = True
                
                from models.classification import db
                db.session.commit()
                print("âœ… äººå·¥è¯„ä¼°æ›´æ–°åŠŸèƒ½æ­£å¸¸")
                
                # æ¢å¤åŸçŠ¶
                evaluation.human_total_score = None
                evaluation.human_reasoning = None
                evaluation.human_evaluation_by = None
                evaluation.human_evaluation_time = None
                evaluation.is_human_modified = False
                db.session.commit()
                
            except Exception as e:
                print(f"âŒ äººå·¥è¯„ä¼°æ›´æ–°å¤±è´¥: {e}")
                return False
            
            return True
            
    except Exception as e:
        print(f"âŒ éªŒè¯äººå·¥è¯„ä¼°APIå¤±è´¥: {e}")
        return False

def check_evaluation_statistics():
    """æ£€æŸ¥è¯„ä¼°ç»Ÿè®¡åŠŸèƒ½"""
    print("\nğŸ“Š æ£€æŸ¥è¯„ä¼°ç»Ÿè®¡åŠŸèƒ½...")
    
    try:
        from app import app
        from services.evaluation_history_service import EvaluationHistoryService
        
        with app.app_context():
            service = EvaluationHistoryService()
            
            # æµ‹è¯•è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = service.get_evaluation_statistics()
            
            if stats['success']:
                print("âœ… è¯„ä¼°ç»Ÿè®¡åŠŸèƒ½æ­£å¸¸")
                print(f"  æ€»è¯„ä¼°æ•°: {stats['data']['total_evaluations']}")
                return True
            else:
                print(f"âŒ è¯„ä¼°ç»Ÿè®¡åŠŸèƒ½å¤±è´¥: {stats['message']}")
                return False
                
    except Exception as e:
        print(f"âŒ æ£€æŸ¥è¯„ä¼°ç»Ÿè®¡åŠŸèƒ½å¤±è´¥: {e}")
        return False

def create_test_evaluation():
    """åˆ›å»ºæµ‹è¯•è¯„ä¼°è®°å½•"""
    print("\nğŸ§ª åˆ›å»ºæµ‹è¯•è¯„ä¼°è®°å½•...")
    
    try:
        from app import app
        from models.classification import db, EvaluationHistory
        
        with app.app_context():
            # åˆ›å»ºæµ‹è¯•è®°å½•
            test_data = {
                'user_input': 'æµ‹è¯•é—®é¢˜ï¼šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ',
                'model_answer': 'äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„ç³»ç»Ÿã€‚',
                'reference_answer': 'äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯ä½¿æœºå™¨èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„æŠ€æœ¯ã€‚',
                'question_time': datetime.utcnow(),
                'evaluation_criteria': 'æµ‹è¯•è¯„ä¼°æ ‡å‡†',
                'total_score': 7.5,
                'dimensions': {'å‡†ç¡®æ€§': 4, 'å®Œæ•´æ€§': 3, 'æµç•…æ€§': 3},
                'reasoning': 'è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•è¯„ä¼°è®°å½•',
                'classification_level1': 'ä¿¡æ¯æŸ¥è¯¢',
                'classification_level2': 'é€šç”¨æŸ¥è¯¢',
                'classification_level3': 'é€šç”¨æŸ¥è¯¢',
                'evaluation_time_seconds': 2.5,
                'model_used': 'deepseek-chat',
                'raw_response': 'æµ‹è¯•åŸå§‹å“åº”'
            }
            
            evaluation = EvaluationHistory.from_dict(test_data)
            db.session.add(evaluation)
            db.session.commit()
            
            print(f"âœ… åˆ›å»ºæµ‹è¯•è®°å½•æˆåŠŸï¼ŒID: {evaluation.id}")
            
            # æµ‹è¯•äººå·¥è¯„ä¼°æ›´æ–°
            evaluation.human_total_score = 8.0
            evaluation.human_reasoning = "äººå·¥è¯„ä¼°ï¼šå›ç­”è´¨é‡å¾ˆå¥½ï¼Œå‡†ç¡®æ€§é«˜"
            evaluation.human_evaluation_by = "æµ‹è¯•ä¸“å®¶"
            evaluation.human_evaluation_time = datetime.utcnow()
            evaluation.is_human_modified = True
            
            db.session.commit()
            print("âœ… äººå·¥è¯„ä¼°æ›´æ–°æˆåŠŸ")
            
            # éªŒè¯æ•°æ®
            data = evaluation.to_dict()
            print(f"âœ… æ•°æ®åºåˆ—åŒ–æˆåŠŸï¼ŒID: {data['id']}")
            
            return evaluation.id
            
    except Exception as e:
        print(f"âŒ åˆ›å»ºæµ‹è¯•è¯„ä¼°è®°å½•å¤±è´¥: {e}")
        return None

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ è¿œç«¯ç¯å¢ƒé—®é¢˜ä¿®å¤å·¥å…·")
    print("=" * 50)
    
    issues_found = []
    
    # 1. æ£€æŸ¥æ•°æ®åº“ç»“æ„
    if not check_database_structure():
        issues_found.append("æ•°æ®åº“ç»“æ„é—®é¢˜")
    
    # 2. ä¿®å¤æ—¥æœŸæ ¼å¼é—®é¢˜
    if not fix_date_format_issues():
        issues_found.append("æ—¥æœŸæ ¼å¼é—®é¢˜")
    
    # 3. éªŒè¯äººå·¥è¯„ä¼°API
    if not verify_human_evaluation_api():
        issues_found.append("äººå·¥è¯„ä¼°APIé—®é¢˜")
    
    # 4. æ£€æŸ¥è¯„ä¼°ç»Ÿè®¡
    if not check_evaluation_statistics():
        issues_found.append("è¯„ä¼°ç»Ÿè®¡é—®é¢˜")
    
    # 5. åˆ›å»ºæµ‹è¯•è®°å½•
    test_id = create_test_evaluation()
    if not test_id:
        issues_found.append("æµ‹è¯•è®°å½•åˆ›å»ºé—®é¢˜")
    
    # æ€»ç»“
    print("\n" + "=" * 50)
    print("ğŸ¯ ä¿®å¤ç»“æœæ€»ç»“")
    print("=" * 50)
    
    if not issues_found:
        print("ğŸ‰ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼è¿œç«¯ç¯å¢ƒé—®é¢˜å·²ä¿®å¤ã€‚")
        if test_id:
            print(f"\nğŸ“‹ æµ‹è¯•è®°å½•ID: {test_id}")
            print("æ‚¨å¯ä»¥åœ¨å‰ç«¯å°è¯•å¯¹è¿™æ¡è®°å½•è¿›è¡Œäººå·¥è¯„ä¼°ã€‚")
    else:
        print(f"âš ï¸  å‘ç° {len(issues_found)} ä¸ªé—®é¢˜ï¼š")
        for issue in issues_found:
            print(f"  - {issue}")
        
        print("\nğŸ’¡ å»ºè®®æ“ä½œï¼š")
        print("1. è¿è¡Œæ•°æ®åº“åˆå§‹åŒ–: python quick_init.py")
        print("2. é‡æ–°åˆ›å»ºæ•°æ®åº“: python database/init_db.py")
        print("3. æ£€æŸ¥Flaskåº”ç”¨é…ç½®")
    
    return len(issues_found) == 0

if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1) 
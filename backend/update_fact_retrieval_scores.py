#!/usr/bin/env python3
"""
æ›´æ–°"äº‹å®æ£€ç´¢"ç±»å‹é—®é¢˜çš„ç»´åº¦è¯„åˆ†
åˆ é™¤"å†…å®¹å®Œæ•´æ€§"å’Œ"æŠ•èµ„å»ºè®®åˆè§„æ€§"è¿™ä¸¤ä¸ªç»´åº¦çš„è¯„åˆ†
"""

import sqlite3
import json
import sys
import os
from datetime import datetime

def connect_database():
    """è¿æ¥æ•°æ®åº“"""
    db_path = 'database/qa_evaluation.db'
    if not os.path.exists(db_path):
        print(f"âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}")
        return None
    
    try:
        conn = sqlite3.connect(db_path)
        print(f"âœ… æˆåŠŸè¿æ¥æ•°æ®åº“: {db_path}")
        return conn
    except Exception as e:
        print(f"âŒ è¿æ¥æ•°æ®åº“å¤±è´¥: {str(e)}")
        return None

def check_fact_retrieval_records(conn):
    """æ£€æŸ¥äº‹å®æ£€ç´¢ç±»å‹çš„è®°å½•"""
    try:
        cursor = conn.cursor()
        
        # æŸ¥è¯¢äº‹å®æ£€ç´¢åˆ†ç±»çš„è®°å½•
        cursor.execute("""
            SELECT id, classification_level2, dimensions_json, total_score, created_at
            FROM evaluation_history 
            WHERE classification_level2 = 'äº‹å®æ£€ç´¢' 
            OR classification_level2 LIKE '%äº‹å®%æ£€ç´¢%'
            ORDER BY created_at DESC
        """)
        
        records = cursor.fetchall()
        
        if not records:
            print("ğŸ“­ æœªæ‰¾åˆ°äº‹å®æ£€ç´¢ç±»å‹çš„è®°å½•")
            
            # æŸ¥çœ‹æ‰€æœ‰å¯èƒ½ç›¸å…³çš„åˆ†ç±»
            cursor.execute("""
                SELECT DISTINCT classification_level2, COUNT(*) as count
                FROM evaluation_history 
                WHERE classification_level2 LIKE '%äº‹å®%' 
                OR classification_level2 LIKE '%æ£€ç´¢%'
                OR classification_level2 LIKE '%æŒ‡æ ‡%'
                GROUP BY classification_level2
                ORDER BY count DESC
            """)
            
            similar_records = cursor.fetchall()
            if similar_records:
                print("ğŸ” æ‰¾åˆ°ç±»ä¼¼çš„åˆ†ç±»:")
                for cat, count in similar_records:
                    print(f"   - {cat}: {count}æ¡è®°å½•")
            
            return []
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(records)} æ¡äº‹å®æ£€ç´¢è®°å½•:")
        
        # åˆ†æç»´åº¦ä¿¡æ¯
        dimension_stats = {}
        for i, (record_id, classification, dimensions_json, total_score, created_at) in enumerate(records[:10]):
            print(f"\nğŸ“‹ è®°å½• #{record_id}:")
            print(f"   - åˆ†ç±»: {classification}")
            print(f"   - æ€»åˆ†: {total_score}")
            print(f"   - åˆ›å»ºæ—¶é—´: {created_at}")
            
            if dimensions_json:
                try:
                    dimensions = json.loads(dimensions_json)
                    print(f"   - ç»´åº¦è¯„åˆ†:")
                    for dim_key, dim_score in dimensions.items():
                        print(f"     * {dim_key}: {dim_score}")
                        dimension_stats[dim_key] = dimension_stats.get(dim_key, 0) + 1
                except json.JSONDecodeError:
                    print(f"   - ç»´åº¦æ•°æ®æ ¼å¼é”™è¯¯")
            else:
                print(f"   - æ— ç»´åº¦æ•°æ®")
        
        if len(records) > 10:
            print(f"\n... (è¿˜æœ‰ {len(records) - 10} æ¡è®°å½•)")
        
        # ç»Ÿè®¡ç»´åº¦ä½¿ç”¨æƒ…å†µ
        if dimension_stats:
            print(f"\nğŸ“ˆ ç»´åº¦ä½¿ç”¨ç»Ÿè®¡:")
            for dim_name, count in sorted(dimension_stats.items(), key=lambda x: x[1], reverse=True):
                print(f"   - {dim_name}: {count}æ¬¡")
        
        return records
        
    except Exception as e:
        print(f"âŒ æ£€æŸ¥è®°å½•å¤±è´¥: {str(e)}")
        return []

def update_fact_retrieval_scores(conn, dry_run=True):
    """æ›´æ–°äº‹å®æ£€ç´¢è®°å½•çš„ç»´åº¦è¯„åˆ†"""
    try:
        cursor = conn.cursor()
        
        # è¦åˆ é™¤çš„ç»´åº¦
        dimensions_to_remove = ['å†…å®¹å®Œæ•´æ€§', 'æŠ•èµ„å»ºè®®åˆè§„æ€§']
        
        # æŸ¥è¯¢éœ€è¦æ›´æ–°çš„è®°å½•
        cursor.execute("""
            SELECT id, dimensions_json, total_score
            FROM evaluation_history 
            WHERE classification_level2 = 'äº‹å®æ£€ç´¢' 
            OR classification_level2 LIKE '%äº‹å®%æ£€ç´¢%'
        """)
        
        records = cursor.fetchall()
        
        if not records:
            print("ğŸ“­ æ²¡æœ‰éœ€è¦æ›´æ–°çš„è®°å½•")
            return
        
        print(f"ğŸ”„ å‡†å¤‡æ›´æ–° {len(records)} æ¡è®°å½•")
        
        updated_count = 0
        skipped_count = 0
        
        for record_id, dimensions_json, current_total_score in records:
            if not dimensions_json:
                print(f"â­ï¸  è·³è¿‡è®°å½• #{record_id}: æ— ç»´åº¦æ•°æ®")
                skipped_count += 1
                continue
            
            try:
                dimensions = json.loads(dimensions_json)
                original_dimensions = dimensions.copy()
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«è¦åˆ é™¤çš„ç»´åº¦
                has_target_dimensions = any(dim in dimensions for dim in dimensions_to_remove)
                
                if not has_target_dimensions:
                    print(f"â­ï¸  è·³è¿‡è®°å½• #{record_id}: ä¸åŒ…å«ç›®æ ‡ç»´åº¦")
                    skipped_count += 1
                    continue
                
                # åˆ é™¤æŒ‡å®šç»´åº¦
                removed_dimensions = []
                for dim_name in dimensions_to_remove:
                    if dim_name in dimensions:
                        removed_score = dimensions.pop(dim_name)
                        removed_dimensions.append(f"{dim_name}({removed_score}åˆ†)")
                
                if not removed_dimensions:
                    print(f"â­ï¸  è·³è¿‡è®°å½• #{record_id}: æ— éœ€åˆ é™¤çš„ç»´åº¦")
                    skipped_count += 1
                    continue
                
                # é‡æ–°è®¡ç®—æ€»åˆ†
                new_total_score = sum(dimensions.values()) if dimensions else 0.0
                
                print(f"ğŸ”§ æ›´æ–°è®°å½• #{record_id}:")
                print(f"   - åˆ é™¤ç»´åº¦: {', '.join(removed_dimensions)}")
                print(f"   - åŸæ€»åˆ†: {current_total_score} â†’ æ–°æ€»åˆ†: {new_total_score}")
                print(f"   - å‰©ä½™ç»´åº¦: {list(dimensions.keys())}")
                
                if not dry_run:
                    # æ›´æ–°æ•°æ®åº“
                    new_dimensions_json = json.dumps(dimensions, ensure_ascii=False)
                    cursor.execute("""
                        UPDATE evaluation_history 
                        SET dimensions_json = ?, total_score = ?, updated_at = ?
                        WHERE id = ?
                    """, (new_dimensions_json, new_total_score, datetime.utcnow(), record_id))
                    
                    print(f"âœ… è®°å½• #{record_id} æ›´æ–°å®Œæˆ")
                else:
                    print(f"ğŸ” [é¢„è§ˆæ¨¡å¼] è®°å½• #{record_id} å°†è¢«æ›´æ–°")
                
                updated_count += 1
                
            except json.JSONDecodeError as e:
                print(f"âŒ è®°å½• #{record_id} JSONè§£æå¤±è´¥: {str(e)}")
                skipped_count += 1
                continue
        
        if not dry_run:
            conn.commit()
            print(f"\nâœ… æ‰¹é‡æ›´æ–°å®Œæˆ!")
        else:
            print(f"\nğŸ” é¢„è§ˆæ¨¡å¼å®Œæˆ!")
        
        print(f"ğŸ“Š ç»Ÿè®¡ç»“æœ:")
        print(f"   - æ›´æ–°è®°å½•æ•°: {updated_count}")
        print(f"   - è·³è¿‡è®°å½•æ•°: {skipped_count}")
        print(f"   - æ€»è®°å½•æ•°: {len(records)}")
        
    except Exception as e:
        print(f"âŒ æ›´æ–°è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {str(e)}")
        conn.rollback()

def backup_database():
    """å¤‡ä»½æ•°æ®åº“"""
    try:
        import shutil
        source = 'database/qa_evaluation.db'
        backup_name = f'database/qa_evaluation_backup_{datetime.now().strftime("%Y%m%d_%H%M%S")}.db'
        
        shutil.copy2(source, backup_name)
        print(f"âœ… æ•°æ®åº“å·²å¤‡ä»½åˆ°: {backup_name}")
        return True
    except Exception as e:
        print(f"âŒ å¤‡ä»½å¤±è´¥: {str(e)}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ äº‹å®æ£€ç´¢ç»´åº¦è¯„åˆ†æ›´æ–°å·¥å…·")
    print("=" * 50)
    
    # è¿æ¥æ•°æ®åº“
    conn = connect_database()
    if not conn:
        return
    
    try:
        # æ£€æŸ¥ç°æœ‰è®°å½•
        print("\nğŸ“‹ ç¬¬ä¸€æ­¥: æ£€æŸ¥ç°æœ‰è®°å½•")
        records = check_fact_retrieval_records(conn)
        
        if not records:
            print("æ²¡æœ‰éœ€è¦å¤„ç†çš„è®°å½•")
            return
        
        # è¯¢é—®æ˜¯å¦ç»§ç»­
        print(f"\nâ“ å‘ç° {len(records)} æ¡äº‹å®æ£€ç´¢è®°å½•")
        print("å°†åˆ é™¤ä»¥ä¸‹ç»´åº¦çš„è¯„åˆ†:")
        print("   - å†…å®¹å®Œæ•´æ€§")
        print("   - æŠ•èµ„å»ºè®®åˆè§„æ€§")
        
        if len(sys.argv) > 1 and sys.argv[1] == '--execute':
            execute = True
            print("ğŸš€ æ‰§è¡Œæ¨¡å¼: å°†ç›´æ¥è¿›è¡Œæ›´æ–°")
        else:
            print("\nğŸ” é¢„è§ˆæ¨¡å¼: ä¸ä¼šå®é™…ä¿®æ”¹æ•°æ®åº“")
            print("ğŸ’¡ å¦‚éœ€å®é™…æ‰§è¡Œï¼Œè¯·ä½¿ç”¨: python update_fact_retrieval_scores.py --execute")
            execute = False
        
        # å¦‚æœæ˜¯æ‰§è¡Œæ¨¡å¼ï¼Œå…ˆå¤‡ä»½
        if execute:
            print("\nğŸ’¾ ç¬¬äºŒæ­¥: å¤‡ä»½æ•°æ®åº“")
            if not backup_database():
                print("âŒ å¤‡ä»½å¤±è´¥ï¼Œç»ˆæ­¢æ“ä½œ")
                return
        
        # æ‰§è¡Œæ›´æ–°
        print(f"\nğŸ”„ ç¬¬ä¸‰æ­¥: {'æ‰§è¡Œæ›´æ–°' if execute else 'é¢„è§ˆæ›´æ–°'}")
        update_fact_retrieval_scores(conn, dry_run=not execute)
        
    finally:
        conn.close()
        print("\nğŸ” æ•°æ®åº“è¿æ¥å·²å…³é—­")

if __name__ == '__main__':
    main() 